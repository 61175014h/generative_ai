{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpA5GJ6SrvtkhHGnbSeux2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cy643/generative_ai/blob/main/0304_hw3/0304_hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 主題二\n",
        "用自己的方式解釋Cross Entropy、KL divergence\n",
        "\n",
        "## 1. GAN的基本原理\n",
        "GAN由兩個主要部分組成：生成器（Generator）和判別器（Discriminator）。\n",
        "- 生成器：生成與真實數據相似的假數據，目標是欺騙判別器。\n",
        "- 判別器：區分真實數據和生成器生成的假數據，目標是準確區分真假數據。\n",
        "通過對抗訓練來提升各自的性能。\n",
        "\n",
        "## 2. 交叉熵（Cross Entropy）在GAN中的應用\n",
        "通常是判別器的損失函數，目標是最小化交叉熵，以便區分真實數據和生成數據。\n",
        "假設一個二分類問題，判別器輸出一個機率值 D(x)，表示輸入 x 是真實數據的機率。交叉熵損失函數可以表示為：\n",
        "```Shell\n",
        "Cross Entropy = -SUM[ylog(D(x)) + (1 - y)log(1 - D(x))]\n",
        "```\n",
        "其中 y 是真實標籤（1表示真實數據，0表示生成數據）。\n",
        "\n",
        "## 3. KL散度（KL Divergence）在GAN中的應用\n",
        "用來衡量生成數據分布和真實數據分布之間的差異，目標是最小化這個差異，使生成數據分布接近真實數據分布。KL散度的公式是：\n",
        "```Shell\n",
        "Dkl(P||Q) = SUM(P(x)log(P(x)/Q(x)))\n",
        "```\n",
        "其中 P 是真實數據分布，Q 是生成數據分布。\n",
        "\n",
        "## 4. 實際計算和效果比較"
      ],
      "metadata": {
        "id": "jIWS5vbLuvNW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "22rgP02Nuera",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08b00a8c-ed88-471d-d2b9-ac7ca4f5c515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Entropy Loss: 1.0216512475319814\n",
            "KL Divergence: 0.08228287850505178\n"
          ]
        }
      ],
      "source": [
        "# 固定套件\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "P = np.array([0.7, 0.3]) # 真實數據機率分佈\n",
        "Q = np.array([0.5, 0.5]) # 生成數據機率分佈\n",
        "D_x = np.array([0.6, 0.4]) # Discriminator Output\n",
        "y = np.array([1, 0]) # Real Label\n",
        "\n",
        "# Cross Entropy Calculation\n",
        "cross_entropy = -np.sum(y * np.log(D_x) + (1 - y) * np.log(1 - D_x))\n",
        "print(\"Cross Entropy Loss:\", cross_entropy)\n",
        "\n",
        "# KL Divergence Calculation\n",
        "kl_divergence = np.sum(P * np.log(P / Q))\n",
        "print(\"KL Divergence:\", kl_divergence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 使用情境\n",
        "- 交叉熵：適用於分類問題(區分真假數據)。\n",
        "- KL散度：適用於衡量兩個機率分布之間的差異(使生成數據分布接近真實數據分布)。\n",
        "\n",
        "## 6. 總結\n",
        "- 交叉熵：用於判別器的損失函數，衡量預測結果和實際結果之間的差距。\n",
        "- KL散度：用於衡量生成數據分布和真實數據分布之間的差異，強調訊息量的損失。"
      ],
      "metadata": {
        "id": "SBj09-gXq-zm"
      }
    }
  ]
}