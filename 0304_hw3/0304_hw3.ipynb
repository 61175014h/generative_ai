{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO57ryhtZZPR+Ykl13CIdPf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cy643/generative_ai/blob/main/0304_hw3/0304_hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 主題二\n",
        "用自己的方式解釋Cross Entropy、KL divergence\n",
        "\n",
        "## 1. GAN的基本原理\n",
        "GAN由兩個主要部分組成：生成器（Generator）和判別器（Discriminator）。\n",
        "- 生成器：生成與真實數據相似的假數據，目標是欺騙判別器。\n",
        "- 判別器：區分真實數據和生成器生成的假數據，目標是準確區分真假數據。\n",
        "通過對抗訓練來提升各自的性能。\n",
        "\n",
        "## 2. 交叉熵（Cross Entropy）在GAN中的應用\n",
        "通常是判別器的損失函數，目標是最小化交叉熵，以便區分真實數據和生成數據。\n",
        "假設 P(x) 是真實數據機率分布，Q(x) 是生成數據機率分布。\n",
        "交叉熵損失函數可以表示為：\n",
        "```Shell\n",
        "H(p,Q) = -SUM[P(x)log(Q(x))]\n",
        "```\n",
        "其中 y 是真實標籤（1表示真實數據，0表示生成數據）。\n",
        "\n",
        "## 3. KL散度（KL Divergence）在GAN中的應用\n",
        "用來衡量生成數據分布和真實數據分布之間的差異，目標是最小化這個差異，使生成數據分布接近真實數據分布。\n",
        "假設 P(x) 是真實數據機率分布，Q(x) 是生成數據機率分布。\n",
        "KL散度的公式是：\n",
        "```Shell\n",
        "DkL(P||Q) = SUM[P(x)log(P(x)/Q(x))]\n",
        "```\n",
        "\n",
        "\n",
        "## 4. 相關性\n",
        "```Shell\n",
        "H(P, Q) = H(P) + DkL(P||Q)\n",
        "```\n",
        "H(P)為P(x)的熵數`H(P) = -SUM[P(x)log(P(x))]`\n",
        "\n",
        "交叉熵 H(P,Q) 可以分解為兩部分：\n",
        "1. **H(P)**： 這是P(x)本身的不確定性\n",
        "2. **DkL(P∣∣Q)**： Q(x) 相對於真實分布 P(x) 訊息量損失。\n",
        "\n",
        "若Q(x)=P(x), DkL = 0, H(P||Q) = H(P)\n",
        "## 5. 實際計算和效果比較"
      ],
      "metadata": {
        "id": "jIWS5vbLuvNW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "22rgP02Nuera",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e7eba95-7a90-4848-809f-bb32f576ab43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P Entropy: 0.6108643020548935\n",
            "KL Divergence: 0.08228287850505178\n",
            "Cross Entropy: 0.6931471805599453\n",
            "H(P, Q) = 0.6931471805599453\n"
          ]
        }
      ],
      "source": [
        "# 固定套件\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "P = np.array([0.7, 0.3]) # 真實數據機率分佈\n",
        "Q = np.array([0.5, 0.5]) # 生成數據機率分佈\n",
        "D_x = np.array([0.6, 0.4]) # Discriminator Output\n",
        "y = np.array([1, 0]) # Real Label\n",
        "\n",
        "# P Entropy Calculation\n",
        "p_entropy = -np.sum(P * np.log(P))\n",
        "print(\"P Entropy:\", p_entropy)\n",
        "\n",
        "# KL Divergence Calculation\n",
        "kl_divergence = np.sum(P * np.log(P / Q))\n",
        "print(\"KL Divergence:\", kl_divergence)\n",
        "\n",
        "# Cross Entropy Calculation\n",
        "cross_entropy = -np.sum(P * np.log(Q))\n",
        "print(\"Cross Entropy:\", cross_entropy)\n",
        "\n",
        "# Verify\n",
        "print(\"H(P, Q) =\", p_entropy + kl_divergence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 執行結果  \n",
        "Cross Entropy = 0.6931.. (Q越接近P，值越小)  \n",
        "KL Divergence = 0.0822.. (學得越好，值越小)"
      ],
      "metadata": {
        "id": "L14mBXW9zhuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 使用情境\n",
        "- 交叉熵：適用於分類問題(區分真假數據)。\n",
        "- KL散度：適用於衡量兩個機率分布之間的差異(使生成數據分布接近真實數據分布)。\n",
        "\n",
        "## 7. 總結\n",
        "- 交叉熵：用於判別器的損失函數，**衡量預測結果和實際結果之間的差距。**\n",
        "- KL散度：**用於衡量生成數據分布和真實數據分布之間的差異**，強調訊息量的損失。"
      ],
      "metadata": {
        "id": "SBj09-gXq-zm"
      }
    }
  ]
}