# 主題二
用自己的方式解釋Cross Entropy、KL divergence

## 1. GAN的基本原理
GAN由兩個主要部分組成：生成器（Generator）和判別器（Discriminator）。
- 生成器：生成與真實數據相似的假數據，目標是欺騙判別器。
- 判別器：區分真實數據和生成器生成的假數據，目標是準確區分真假數據。
通過對抗訓練來提升各自的性能。

## 2. 交叉熵（Cross Entropy）在GAN中的應用
通常是判別器的損失函數，目標是最小化交叉熵，以便區分真實數據和生成數據。
假設 P(x) 是真實數據機率分布，Q(x) 是生成數據機率分布。
交叉熵損失函數可以表示為：
```Shell
H(p,Q) = -SUM[P(x)log(Q(x))]
```
其中 y 是真實標籤（1表示真實數據，0表示生成數據）。

## 3. KL散度（KL Divergence）在GAN中的應用
用來衡量生成數據分布和真實數據分布之間的差異，目標是最小化這個差異，使生成數據分布接近真實數據分布。
假設 P(x) 是真實數據機率分布，Q(x) 是生成數據機率分布。
KL散度的公式是：
```Shell
DkL(P||Q) = SUM[P(x)log(P(x)/Q(x))]
```

## 4. 相關性
```Shell
H(P, Q) = H(P) + DkL(P||Q)
```
H(P)為P(x)的熵數`H(P) = -SUM[P(x)log(P(x))]`

交叉熵 H(P,Q) 可以分解為兩部分：
1. **H(P)**： 這是P(x)本身的不確定性
2. **DkL(P∣∣Q)**： Q(x) 相對於真實分布 P(x) 訊息量損失。

若Q(x)=P(x), DkL = 0, H(P||Q) = H(P)

## 5. 實際計算和效果比較
![Screenshot from 2025-03-10 14-52-26](https://github.com/user-attachments/assets/df8a1ce0-35a0-4ffa-835f-8d9a75dd6c94)
- 執行結果  
Cross Entropy = 0.6931.. (Q越接近P，值越小)  
KL Divergence = 0.0822.. (學得越好，值越小)

## 6. 使用情境
- 交叉熵：適用於分類問題(區分真假數據)。
- KL散度：適用於衡量兩個機率分布之間的差異(使生成數據分布接近真實數據分布)。

## 7. 總結
- 交叉熵：用於判別器的損失函數，**衡量預測結果和實際結果之間的差距。**
- KL散度：**用於衡量生成數據分布和真實數據分布之間的差異**，強調訊息量的損失。
